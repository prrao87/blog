{
  
    
        "post0": {
            "title": "Scalable topic modelling: Comparing Gensim and PySpark",
            "content": ".",
            "url": "https://prrao87.github.io/blog/scalable-topic-modelling/",
            "relUrl": "/scalable-topic-modelling/",
            "date": " • Nov 16, 2020"
        }
        
    
  
    
  
    
  
    
        ,"post3": {
            "title": "Turbo-charge your spaCy NLP pipeline",
            "content": ". Background . Consider you have a large text dataset on which you want to apply some non-trivial NLP transformations, such as stopword removal followed by lemmatizing the words (i.e. reducing them to root form) in the text. spaCy is an industrial strength NLP library designed for just such a task. . In the example shown below, the New York Times dataset is used to showcase how to significantly speed up a spaCy NLP pipeline. The goal is to take in an article&#39;s text, and speedily return a list of lemmas with unnecessary words, i.e. stopwords, removed. . Pandas DataFrames provide a convenient interface to work with tabular data of this nature. First, import the necessary modules shown. . import re import pandas as pd import spacy . . Initial steps . The news data is obtained by running the preprocessing notebook (./data/preprocessing.ipynb), which processes the raw text file downloaded from Kaggle and performs some basic cleaning on it. This step generates a file that contains the tabular data (stored as nytimes.tsv). A curated stopword file is also provided in the same directory. . Additionally, during initial testing, we can limit the size of the DataFrame being worked on (to a subset of the total number of articles) for faster execution. For the final run, disable the limit by setting it to zero. . inputfile = &quot;data/spacy_multiprocess/nytimes-sample.tsv&quot; stopwordfile = &quot;data/spacy_multiprocess/stopwords/stopwords.txt&quot; limit = 0 . . Load spaCy model . Since we will not be doing any specialized tasks such as dependency parsing and named entity recognition in this exercise, these components are disabled when loading the spaCy model. . . Tip: spaCy has a sentencizer component that can be plugged into a blank pipeline. . The sentencizer pipeline simply performs tokenization and sentence boundary detection, following which lemmas can be extracted as token properties. . nlp = spacy.load(&#39;en_core_web_sm&#39;, disable=[&#39;tagger&#39;, &#39;parser&#39;, &#39;ner&#39;]) nlp.add_pipe(nlp.create_pipe(&#39;sentencizer&#39;)) . A method is defined to read in stopwords from a text file and convert it to a set in Python (for efficient lookup). . def get_stopwords(): &quot;Return a set of stopwords read in from a file.&quot; with open(stopwordfile) as f: stopwords = [] for line in f: stopwords.append(line.strip(&quot; n&quot;)) # Convert to set for performance stopwords_set = set(stopwords) return stopwords_set stopwords = get_stopwords() . Read in New York Times Dataset . The pre-processed version of the NYT news dataset is read in as a Pandas DataFrame. The columns are named date, headline and content - the text present in the content column is what will be preprocessed to remove stopwords and generate token lemmas. . def read_data(inputfile): &quot;Read in a tab-separated file with date, headline and news content&quot; df = pd.read_csv(inputfile, sep=&#39; t&#39;, header=None, names=[&#39;date&#39;, &#39;headline&#39;, &#39;content&#39;]) df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], format=&quot;%Y-%m-%d&quot;) return df . df = read_data(inputfile) df.head(3) . date headline content . 0 2016-06-30 | washington nationals max scherzer baffles mets... | Stellar pitching kept the Mets afloat in the f... | . 1 2016-06-30 | mayor de blasios counsel to leave next month t... | Mayor Bill de Blasio’s counsel and chief legal... | . 2 2016-06-30 | three men charged in killing of cuomo administ... | In the early morning hours of Labor Day last y... | . Define text cleaner . Since the news article data comes from a raw HTML dump, it is very messy and contains a host of unnecessary symbols, social media handles, URLs and other artifacts. An easy way to clean it up is to use a regex that parses only alphanumeric strings and hyphens (so as to include hyphenated words) that are between a given length (3 and 50). This filters each document down to only meaningful text for the lemmatizer. . def cleaner(df): &quot;Extract relevant text from DataFrame using a regex&quot; # Regex pattern for only alphanumeric, hyphenated text with 3 or more chars pattern = re.compile(r&quot;[A-Za-z0-9 -]{3,50}&quot;) df[&#39;clean&#39;] = df[&#39;content&#39;].str.findall(pattern).str.join(&#39; &#39;) if limit &gt; 0: return df.iloc[:limit, :].copy() else: return df . df_preproc = cleaner(df) df_preproc.head(3) . date headline content clean . 0 2016-06-30 | washington nationals max scherzer baffles mets... | Stellar pitching kept the Mets afloat in the f... | Stellar pitching kept the Mets afloat the firs... | . 1 2016-06-30 | mayor de blasios counsel to leave next month t... | Mayor Bill de Blasio’s counsel and chief legal... | Mayor Bill Blasio counsel and chief legal advi... | . 2 2016-06-30 | three men charged in killing of cuomo administ... | In the early morning hours of Labor Day last y... | the early morning hours Labor Day last year gr... | . Now that we have just the clean, alphanumeric tokens left over, these can be further cleaned up by removing stopwords before proceeding to lemmatization. . Option 1: Sequentially process DataFrame column . The straightforward way to process this text is to use an existing method, in this case the lemmatize method shown below, and apply it to the clean column of the DataFrame using pandas.Series.apply. Lemmatization is done using the spaCy&#39;s underlying Doc representation of each token, which contains a lemma_ property. Stopwords are removed simultaneously with the lemmatization process, as each of these steps involves iterating through the same list of tokens. . def lemmatize(text): &quot;&quot;&quot;Perform lemmatization and stopword removal in the clean text Returns a list of lemmas &quot;&quot;&quot; doc = nlp(text) lemma_list = [str(tok.lemma_).lower() for tok in doc if tok.is_alpha and tok.text.lower() not in stopwords] return lemma_list . The resulting lemmas are stored as a list in a separate column preproc as shown below. . %%time df_preproc[&#39;preproc&#39;] = df_preproc[&#39;clean&#39;].apply(lemmatize) df_preproc[[&#39;date&#39;, &#39;content&#39;, &#39;preproc&#39;]].head(3) . CPU times: user 48.5 s, sys: 146 ms, total: 48.6 s Wall time: 48.6 s . date content preproc . 0 2016-06-30 | Stellar pitching kept the Mets afloat in the f... | [stellar, pitch, keep, mets, afloat, half, sea... | . 1 2016-06-30 | Mayor Bill de Blasio’s counsel and chief legal... | [mayor, bill, blasio, counsel, chief, legal, a... | . 2 2016-06-30 | In the early morning hours of Labor Day last y... | [early, labor, group, gunman, street, gang, cr... | . Applying this method to the clean column of the DataFrame and timing it shows that it takes almost a minute to run on 8,800 news articles. . Option 2: Use nlp.pipe . Can we do better? in the spaCy documentation, it is stated that &quot;processing texts as a stream is usually more efficient than processing them one-by-one&quot;. This is done by calling a language pipe, which internally divides the data into batches to reduce the number of pure-Python function calls. This means that the larger the data, the better the performance gain that can be obtained by nlp.pipe. . To use the language pipe to stream texts, a new lemmatizer method is defined that directly works on a spaCy Doc object. This method is then called in batches to work on a sequence of Doc objects that are streamed through the pipe as shown below. . def lemmatize_pipe(doc): lemma_list = [str(tok.lemma_).lower() for tok in doc if tok.is_alpha and tok.text.lower() not in stopwords] return lemma_list def preprocess_pipe(texts): preproc_pipe = [] for doc in nlp.pipe(texts, batch_size=20): preproc_pipe.append(lemmatize_pipe(doc)) return preproc_pipe . Just as before, a new column is created by passing data from the clean column of the existing DataFrame. Note that unlike in workflow #1 above, we do not use the apply method here - instead, the column of data (an iterable) is directly passed as an argument to the preprocessor pipe method. . %%time df_preproc[&#39;preproc_pipe&#39;] = preprocess_pipe(df_preproc[&#39;clean&#39;]) df_preproc[[&#39;date&#39;, &#39;content&#39;, &#39;preproc_pipe&#39;]].head(3) . CPU times: user 51.6 s, sys: 144 ms, total: 51.8 s Wall time: 51.8 s . date content preproc_pipe . 0 2016-06-30 | Stellar pitching kept the Mets afloat in the f... | [stellar, pitch, keep, mets, afloat, half, sea... | . 1 2016-06-30 | Mayor Bill de Blasio’s counsel and chief legal... | [mayor, bill, blasio, counsel, chief, legal, a... | . 2 2016-06-30 | In the early morning hours of Labor Day last y... | [early, labor, group, gunman, street, gang, cr... | . Timing this workflow doesn&#39;t seem to show improvement over the previous workflow, but as per the spaCy documentation, one would expect that as we work on bigger and bigger datasets, this approach should show some timing improvement (on average). . Option 3: Parallelize the work using joblib . We can do still better! The previous workflows sequentially worked through each news document to produce the lemma lists, which were then appended to the DataFrame as a new column. Because each row&#39;s output is completely independent of the other, this is an embarassingly parallel problem, making it ideal for using multiple cores. . The joblib library is recommended by spaCy for processing blocks of an NLP pipeline in parallel. Make sure that you pip install joblib before running the below section. . To parallelize the workflow, a few more helper methods must be defined. . Chunking: The news article content is a list of (long) strings where each document represents a single article&#39;s text. This data must be fed in &quot;chunks&quot; to each worker process started by joblib. Each call of the chunker method returns a generator that only contains that particular chunk&#39;s text as a list of strings. During lemmatization, each new chunk is retrieved based on the iterator index (with the previous chunks being &quot;forgotten&quot;). | . Flattening: Once joblib creates a set of worker processes that work on each chunk, each worker returns a &quot;list of lists&quot; containing lemmas for each document. These lists are then combined by the executor to provide a 3-level nested final &quot;list of lists of lists&quot;. To ensure that the length of the output from the executor is the same as the actual number of articles, a &quot;flatten&quot; method is defined to combine the result into a list of lists containing lemmas. As an example, two parallel executors would return a final nested list of the form: [[[a, b, c], [d, e, f]], [[g, h, i], [j, k, l]]], where [[a, b, c], [d, e, f]] and [[g, h, i], [j, k, l]] refer to the output from each executor (the final output is then concatenated to a single list by joblib). A flattened version of this result would be [[a, b, c], [d, e, f], [g, h, i], [j, k, l]], i.e. with one level of nesting removed. | . In addition to the above methods, a similar nlp.pipe method is used as in workflow #2, on each chunk of texts. Each of these methods is wrapped into a preprocess_parallel method that defines the number of worker processes to be used (7 in this case), breaks the input data into chunks and returns a flattened result that can then be appended to the DataFrame. For machine with a higher number of physical cores, the number of worker processes can be increased further. . The parallelized workflow using joblib is shown below. . from joblib import Parallel, delayed def chunker(iterable, total_length, chunksize): return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize)) def flatten(list_of_lists): &quot;Flatten a list of lists to a combined list&quot; return [item for sublist in list_of_lists for item in sublist] def process_chunk(texts): preproc_pipe = [] for doc in nlp.pipe(texts, batch_size=20): preproc_pipe.append(lemmatize_pipe(doc)) return preproc_pipe def preprocess_parallel(texts, chunksize=100): executor = Parallel(n_jobs=7, backend=&#39;multiprocessing&#39;, prefer=&quot;processes&quot;) do = delayed(process_chunk) tasks = (do(chunk) for chunk in chunker(texts, len(df_preproc), chunksize=chunksize)) result = executor(tasks) return flatten(result) . %%time df_preproc[&#39;preproc_parallel&#39;] = preprocess_parallel(df_preproc[&#39;clean&#39;], chunksize=1000) . CPU times: user 683 ms, sys: 248 ms, total: 932 ms Wall time: 17.2 s . df_preproc[[&#39;date&#39;, &#39;content&#39;, &#39;preproc_parallel&#39;]].head(3) . date content preproc_parallel . 0 2016-06-30 | Stellar pitching kept the Mets afloat in the f... | [stellar, pitch, keep, mets, afloat, half, sea... | . 1 2016-06-30 | Mayor Bill de Blasio’s counsel and chief legal... | [mayor, bill, blasio, counsel, chief, legal, a... | . 2 2016-06-30 | In the early morning hours of Labor Day last y... | [early, labor, group, gunman, street, gang, cr... | . Timing this parallelized workflow shows significant performance gains (almost 3x reduction in run time)! As the number of documents becomes larger, the additional overhead of starting multiple worker threads with joblib is quickly paid for, and this method can significantly outperform the sequential methods. . Effect of chunk size and batch size . Note that in the parallelized workflow, two parameters need to be specified - the optimum number can vary depending on the dataset. The chunksize controls the size of each chunk being worked on by each process. In this example, for 8,800 documents, a chunksize of 1000 is used. Too small a chunksize would mean that a large number of worker threads would spawn to deal with the large number of chunks overall, which can slow down execution. Generally, a chunksize of several hundred documents to a few thousand is a good starting point (of course, this depends on how big each document in the data is so that the chunks can fit into memory). . The batch size is parameter specific to nlp.pipe, and again, a good value depends on the data being worked on. For reasonably long-sized text such as news articles, it makes sense to keep the batch size reasonably small (so that each batch doesn&#39;t contain really long texts), so in this case 20 was chosen for the batch size. For other cases (e.g. Tweets) where each document is much shorter in length, a larger batch size can be used. . It is recommended to experiment with either parameter to see which combination produces the best performance. . Sets vs. Lists . . Important: Use sets over lists for lookups wherever possible. . Note that in the get_stopwords() method defined earlier on, the list of stopwords read in from the stopword file was converted to a set before using it in the lemmatizer method for stopword removal via lookups. This is a very useful trick in general, but specifically for stopword removal, the use of sets becomes all the more important. Why? . In any realistic stopword list, such as this one for a news dataset, it&#39;s reasonable to expect several hundred stopwords. This is because for downstream tasks such as topic modelling or sentiment analysis, there are a number of domain-specific words that need to be removed (very common verbs, useless abbreviations such as timezones, days of the week, etc.). Each word in each and every document needs to be compared against every word in the stopword list, which is an expensive operation over tens of thousands of documents. . It&#39;s well known that sets have $O(1)$ (i.e. constant) lookup time as opposed to lists, which have $O(n)$ lookup time. In the lemmatize() method, since we&#39;re checking each word for membership in the set of stopwords, we would expect sets to be much better than lists. To test this, we can rerun workflow #1 but this time, use a stopword list instead. . stopwords = list(stopwords) . %%time df_preproc[&#39;preproc_stopword_list&#39;] = df_preproc[&#39;clean&#39;].apply(lemmatize) df_preproc[[&#39;date&#39;, &#39;content&#39;, &#39;preproc_stopword_list&#39;]].head(3) . CPU times: user 1min 17s, sys: 108 ms, total: 1min 18s Wall time: 1min 18s . date content preproc_stopword_list . 0 2016-06-30 | Stellar pitching kept the Mets afloat in the f... | [stellar, pitch, keep, mets, afloat, half, sea... | . 1 2016-06-30 | Mayor Bill de Blasio’s counsel and chief legal... | [mayor, bill, blasio, counsel, chief, legal, a... | . 2 2016-06-30 | In the early morning hours of Labor Day last y... | [early, labor, group, gunman, street, gang, cr... | . This method now takes ~50% longer than it did before (when using a stopword set), which is a 1.5x increase in run time! This makes sense because in this case, the stopword list is about 500 words long, and each and every word in the corpus needs to be checked for membership in this reasonable-sized list. . Conclusions . In this exercise, a news article dataset (NY Times) was processed using a spaCy pipeline to output a list of lemmas representing the useful tokens present in each article&#39;s content. Because real-world news datasets are almost certainly bigger than this one, and can be unbounded in size, a fast, efficient NLP pipeline is necessary to perform any meaningful analysis on the data. The following steps are very useful in speeding up the spaCy pipeline. . Disable unnecessary components in spaCy model: The standard spaCy model&#39;s pipeline contains the tagger (to assign part-of-speech tags), the parser (to generate a dependency parse) and named entity recognition components. If any or none of these actions are desired, these components must be disabled immediately after loading the model (as shown above). . Use sets over lists for lookups: When performing lookups to compare one set of tokens against another, always perform membership checks using sets - lists are significantly slower for lookups! The larger the list/set of stopwords, the bigger the performance gain seen when using sets. . Use custom language pipes when possible: Setting up a language pipe using nlp.pipe is an extremely flexible and efficient way to process large blocks of text. Even better, spaCy allows you to individually disable components for each specific sub-task, for example, when you need to separately perform part-of-speech tagging and named entity recognition (NER). See the spaCy docs for examples on how to disable pipeline components during model loading, processing or handling custom blocks. . Use multiple cores when possible: When processing individual documents completely independent of one another, consider parallelizing the workflow by passing the computation to multiple cores. As the number of documents becoms higher and higher, the performance gains can be tremendous. One just needs to ensure that the documents are divided up into chunks, all of which must fit into memory at any given time. . I hope this was useful -- have fun testing these out in your next NLP project! .",
            "url": "https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html",
            "relUrl": "/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html",
            "date": " • May 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I’m Hamel Husain. I’m a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I’ve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . We&#39;re launching `fastpages`, a platform which allows you to host a blog for free, with no ads. You can blog with @ProjectJupyter notebooks, @office Word, directly from @github&#39;s markdown editor, etc.Nothing to install, &amp; setup is automated!https://t.co/dNSA0oQUrN . &mdash; Jeremy Howard (@jeremyphoward) February 24, 2020 Machine Learning Ops . Wouldn’t it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | … and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository’s files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let’s take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . First, we define triggers in ci.yaml. Like all Actions workflows, this is a YAML file located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‘Initial commit’. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow’s first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is “Initial commit” as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called fastai/fastpages-jekyll because I’m adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don’t have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP’s AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face’s Actions workflows for machine learning on GitHub &#8617; . | These additional dependencies are defined here, which uses the “jekyll build” command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here. &#8617; . |",
            "url": "https://prrao87.github.io/blog/actions/markdown/2020/03/06/fastpages-actions.html",
            "relUrl": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://prrao87.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://prrao87.github.io/blog/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a data scientist, developer and engineer with a scientific consulting background. My long-term interests are in Natural Language Processing (NLP), knowledge representation/reasoning and large-scale process automation. I am passionate about building intelligent, end-to-end automated systems using a range of machine learning and database technologies. I also enjoy communicating technical concepts to a broad and varied audience. . In the past, I’ve worked on a range of technical problems, from running computational physics simulations, to making complex data visualizations and building end-to-end machine learning workflows. My primary tools of choice are Python, numpy, pandas, PyTorch and Spark. I’m currently fascinated by (and am actively learning) the Julia programming language. .",
          "url": "https://prrao87.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Comment Policy",
          "content": "Thanks for choosing to comment! Here are some basic guidelines: . Please be polite - a civil discussion can yield fruitful outcomes for both parties. | Don’t troll - comments that involve personal attacks or otherwise rude behaviour will be removed. | NO spam please - nothing to do with making money, visiting your new fancy website or other non-blog-related endorsements. | . “Few people are capable of expressing with equanimity opinions which differ from the prejudices of their social environment. Most people are incapable of forming such opinions.” - Albert Einstein (Letter to Leo Baeck, 1953) .",
          "url": "https://prrao87.github.io/blog/comment-policy/",
          "relUrl": "/comment-policy/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://prrao87.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}